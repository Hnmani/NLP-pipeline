{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP_pipeline.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmsm9UTsPzH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bc698485-727f-4d8b-bcd5-4ec56dd7d0b6"
      },
      "source": [
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krrYCtugPzIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Importing the required libraries.\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NXsmPoHPzIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##regular expression to identify URLs.\n",
        "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEPPbIOyPzIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def starting_verb(self, text):\n",
        "        # tokenize by sentences \n",
        "        sentence_list = sent_tokenize(text)\n",
        "        \n",
        "        for sentence in sentence_list:\n",
        "            # tokenize each sentence into words and tag part of speech\n",
        "            pos_tags = nltk.pos_tag(word_tokenize(sentence))\n",
        "\n",
        "            # index pos_tags to get the first word and part of speech tag\n",
        "            first_word, first_tag = pos_tags[0]\n",
        "            \n",
        "            # returns true if the first word is an appropriate verb or RT for retweet\n",
        "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
        "                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # apply starting_verb function to all values in X\n",
        "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
        "\n",
        "        return pd.DataFrame(X_tagged)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHc1-wn-PzIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "98f89cd2-f8db-4183-b950-3d3052ed27eb"
      },
      "source": [
        "def load_data():\n",
        "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
        "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
        "    X = df.text.values\n",
        "    y = df.category.values\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    #finds all the urls\n",
        "    detected_urls = re.findall(url_regex, text)\n",
        "    for url in detected_urls:\n",
        "        #replaces the urls with urlplaceholder\n",
        "        text = text.replace(url, \"urlplaceholder\")\n",
        "        \n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean_tokens = []\n",
        "    for tok in tokens:\n",
        "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
        "        clean_tokens.append(clean_tok)\n",
        "\n",
        "    return clean_tokens\n",
        "\n",
        "\n",
        "def model_pipeline():\n",
        "    \"\"\"\n",
        "    The Function returns a Pipeline\n",
        "    Pipelinse structure :\n",
        "    => features: Feature Union |=> text_pipeline: PipeLine |=> vect: CountVectorizer |=> tfidf: TfidfTransformer |=> clf: RandomForestClassifier\n",
        "                               |                                                                                 ||\n",
        "                               |                                                                                 ||\n",
        "                               |=> starting_verb: StartingVerbExtractor |========================================||\n",
        "    \n",
        "    \"\"\"\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "\n",
        "            ('text_pipeline', Pipeline([\n",
        "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "                ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "\n",
        "            ('starting_verb', StartingVerbExtractor())\n",
        "        ])),\n",
        "\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "def display_results(y_test, y_pred):\n",
        "    labels = np.unique(y_pred)\n",
        "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "    accuracy = (y_pred == y_test).mean()\n",
        "\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "def main():\n",
        "    X, y = load_data()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    model = model_pipeline()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    display_results(y_test, y_pred)\n",
        "\n",
        "main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels: ['Action' 'Dialogue' 'Information']\n",
            "Confusion Matrix:\n",
            " [[ 92   0  25]\n",
            " [  0  26   6]\n",
            " [  7   0 445]]\n",
            "Accuracy: 0.9367720465890182\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}